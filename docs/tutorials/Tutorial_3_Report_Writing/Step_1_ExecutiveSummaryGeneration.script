###################################################################################
# Step 1: Executive Summary Generation
# This script creates executive summaries from retrieved documents using RAG.
# It uses the same vector index from Tutorial 2 but focuses on generating
# structured, professional summaries rather than answering specific questions.
#
# The pipeline is:
# 1. Read the topic from the user through a web interface.
# 2. Make a defensive copy of the input data for browser compatibility.
# 3. Use the `llmEmbed` segment to convert the topic into a vector.
# 4. Use the `searchLanceDB` segment to retrieve relevant documents.
# 5. Use the custom `executiveSummaryPrompt` segment to structure the content.
# 6. Generate a professional executive summary using the LLM.
###################################################################################

| copy
| llmEmbed[field="topic", source="ollama", model="mxbai-embed-large", set_as="vector"]
| searchLanceDB[field="vector", path="../Tutorial_2-Search_by_Example_and_RAG/vector_index", table_name="stories", all_results_at_once=True, set_as="results"]
| executiveSummaryPrompt
| llmPrompt[source="ollama", model="llama3.2"]
